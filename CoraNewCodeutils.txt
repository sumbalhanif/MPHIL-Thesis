import torch
from dgl import DGLGraph
from dgl.data import CiteseerGraphDataset
from dgl.data import CoraGraphDataset
from torch.nn import functional as F
import torch.optim as optim
from sklearn.metrics import f1_score
import torch.nn as nn
from torch.nn.utils.rnn import pad_sequence
import torch.nn as nn
import time 
import numpy as np
from scipy.stats import entropy
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt


def load_cora_dataset_dgl():
    # Load the Cora dataset from DGL
    dataset = CoraGraphDataset()
    g = dataset[0]
    x = g.ndata['feat']
    y = g.ndata['label']
    num_features_per_node = x.shape[1]

    #print(f"Number of features per node: {num_features_per_node}")
    node_ids = torch.arange(g.number_of_nodes())
    for node_id in node_ids:
        label = y[node_id].item()
        features = x[node_id].numpy()

        #print(f"Node ID: {node_id}, Label: {label}, Features: {features}")
    train_mask = g.ndata['train_mask']
    val_mask = g.ndata['val_mask']
    test_mask = g.ndata['test_mask']
    return g, x, y, train_mask, val_mask, test_mask


def calculate_f1(model, g, x, labels, mask):
    model.eval()
    output = model(g, x)
    y_pred = output[mask].argmax(dim=1).cpu().numpy()
    y_true = labels[mask].cpu().numpy()
    #print("Predicted Labels:", y_pred)
    #print("true label:", y_true)
    # Print Confusion Matrix
    
    return f1_score(y_true, y_pred, average='micro')


def train_teacher(model, g, x, y, train_mask, optimizer, criterion, val_mask, test_mask, teacher_epochs):
     

    for epoch in range(teacher_epochs):
        model.train()
        optimizer.zero_grad()
        output = model(g, x)
        loss = criterion(output[train_mask], y[train_mask])
        loss.backward()
        optimizer.step()
        print(f'Epoch {epoch + 1:04d} | Loss: {loss.item():.4f}')

        if (epoch + 1) % 10 == 0:
            f1_val = calculate_f1(model, g, x, y, val_mask)
            f1_test = calculate_f1(model, g, x, y, test_mask)
            print(f'F1-Score on valset: {f1_val:.4f} | F1-Score on testset: {f1_test:.4f}')

    
    
    

    # Evaluate the model on the test set
    model.eval()
    test_output = model(g, x)
    test_f1 = calculate_f1(model, g, x, y, test_mask)
    print(f'Test F1 Score: {test_f1:.4f}')
    return loss.item()
def train_student(tofull,mode,student_model, g, x, y, train_mask, optimizer, criterion, temperature, teacher_output, epochs, lambda_ls, beta_gs, val_mask, test_mask, warmup_epochs=5):
    

    for epoch in range(epochs):
        student_model.train()
        optimizer.zero_grad()
        logits = student_model(g, x)
        filtered_y = y[train_mask].view(-1).long()
        logits_masked = logits[train_mask]

        if epoch >=tofull:
                mode = 'full'

                if mode == 'full':
                    
                    ls_loss = torch.tensor(0)
                    gs_loss=torch.tensor(0)
                    ce_loss = nn.CrossEntropyLoss()(logits_masked, filtered_y)
                    loss=ce_loss

        elif epoch<=tofull:
          mode=='mi'
          kd_loss = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(logits_masked / temperature, dim=1),
                                                        F.softmax(teacher_output[train_mask] / temperature, dim=1))
          ce_loss = nn.CrossEntropyLoss()(logits_masked, filtered_y)
          ls_loss = calculate_local_structure_loss(student_model, teacher_output, g, x, train_mask)
          gs_loss = calculate_global_structure_loss(student_model, teacher_output, g, x, train_mask)
          loss = kd_loss + ce_loss + lambda_ls * ls_loss + beta_gs * gs_loss
          #loss = kd_loss + ce_loss + lambda_ls * ls_loss
        else:
          ce_loss = nn.CrossEntropyLoss()(logits_masked, filtered_y)
          loss=ce_loss


        loss.backward()
        optimizer.step()
        student_probs = F.softmax(logits, dim=1)
        print(f'Epoch {epoch + 1:05d} | Loss: {loss.item():.4f} | Local Structure Loss: {ls_loss:.4f} | Global Structure Loss: {gs_loss:.4f}')
        #print(f'Epoch {epoch + 1:05d} | Loss: {loss.item():.4f} | Local Structure Loss: {ls_loss:.4f}')


        if epoch >= warmup_epochs and (epoch + 1) % 10 == 0:
            f1_val = calculate_f1(student_model, g, x, y, val_mask)
            print(f'Epoch {epoch + 1:05d} | F1-Score on valset: {f1_val:.4f}')
            f1_test_improvement = False
            if epoch > 0:
                f1_test_prev = calculate_f1(student_model, g, x, y, test_mask)
                if f1_val > f1_test_prev:
                    f1_test = calculate_f1(student_model, g, x, y, test_mask)
                    f1_test_improvement = True
                    print(f'F1-Score on testset: {f1_test:.4f} (Improved)')
                else:
                    print(f'F1-Score on testset: {f1_test_prev:.4f}')
   
    
    student_model.eval()
    test_output = student_model(g, x)
    test_f1 = calculate_f1(student_model, g, x, y, test_mask)
    print(f'Test F1 Score: {test_f1:.4f}')
    return loss.item(), student_probs, ls_loss, gs_loss

def calculate_local_structure_loss(student_model, teacher_output, g,x, mask, temperature=1.0):
   
    student_embeddings = student_model(g,x)

    student_embeddings = student_embeddings[mask]
    teacher_embeddings = teacher_output[mask]

    soft_labels = F.softmax(teacher_embeddings / temperature, dim=-1)

    normalized_student = F.normalize(student_embeddings, p=2, dim=1)
    normalized_soft_labels = F.normalize(soft_labels, p=2, dim=1)
    cosine_similarity = torch.mm(normalized_student, normalized_soft_labels.t())

    ls_loss = 1 - cosine_similarity.mean() 

    return ls_loss

   
def calculate_global_structure_loss(student_model, teacher_output, g, x, train_mask, temperature=1.0):
    logits = student_model(g, x)
    teacher_logits = teacher_output

    student_gs = F.softmax(logits / temperature, dim=1).detach()  
    teacher_gs = F.softmax(teacher_logits / temperature, dim=1).detach()  

    cosine_similarity = calculate_cosine_similarity(student_gs, teacher_gs)
    gsp_loss = (1.0 - cosine_similarity) / len(g.edges())

    return gsp_loss
def calculate_cosine_similarity(A, B):
    dot_product = torch.sum(A * B)
    norm_A = torch.norm(A)
    norm_B = torch.norm(B)

    cosine_similarity = dot_product / (norm_A * norm_B + 1e-8) 
    return cosine_similarity