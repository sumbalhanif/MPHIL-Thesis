import argparse
import torch
import torch.optim as optim
import torch.nn.functional as F
from sklearn.metrics import f1_score
from utils import train_teacher, train_student, calculate_f1
from gat import GATModel
from utils import calculate_local_structure_loss,calculate_global_structure_loss
from utils import load_cora_dataset_dgl
import time
import random
import numpy as np


def parse_args():
    parser = argparse.ArgumentParser(description="Knowledge Distillation with GAT on Cora dataset")
    parser.add_argument("--teacher-hidden-layers", type=int, default=2, help="Number of hidden layers in the teacher model")
    parser.add_argument("--teacher-hidden-units", type=int, default=254, help="Number of hidden units in each layer of the teacher model")
    parser.add_argument("--teacher-heads", type=int, default=4, help="Number of attention heads in the teacher model")
    parser.add_argument("--teacher-epochs", type=int, default=50, help="Number of training epochs for the teacher model")
    parser.add_argument("--output-layers", type=int, default=1, help="Number of output layers in the teacher model")
    parser.add_argument("---hidden-units-output-layer", type=int, default=7, help="Number of hidden units in each layer of the teacher model")
    



    parser.add_argument("--student-hidden-layers", type=int, default=4, help="Number of hidden layers in the student model")
    parser.add_argument("--student-hidden-units", type=int, default=66, help="Number of hidden units in each layer of the student model")
    parser.add_argument("--student-heads", type=int, default=2, help="Number of attention heads in the student model")
    parser.add_argument("--student-epochs", type=int, default=100, help="Number of training epochs for the student model")

    parser.add_argument("--learning-rate", type=float, default=0.001, help="Learning rate for both teacher and student models")
    parser.add_argument("--temperature", type=float, default=3.0, help="Temperature parameter for knowledge distillation")
    parser.add_argument("--lambda_ls", type=float, default=0.5, help="Weight of local structure preserving loss in the total loss")
    parser.add_argument("--beta_gs", type=float, default=0.5, help="Weight of global structure preserving loss in the total loss")
    parser.add_argument('--tofull', type=int, default=20,help="change mode to full after tofull epochs")
    parser.add_argument("--mode", type=str, default='mi')


    return parser.parse_args()



def main():
    seed = 42
    torch.manual_seed(seed)
    random.seed(seed)
    np.random.seed(seed)

    args = parse_args()

    
    g, x, y, train_mask, val_mask, test_mask = load_cora_dataset_dgl()

    
    teacher_model = GATModel(
    num_features=x.shape[1],
    num_classes=int(y.max().item()) + 1,
    hidden_units=args.teacher_hidden_units,
    num_heads=args.teacher_heads,
    output_layers=args.output_layers,
    hidden_units_output_layer=args.hidden_units_output_layer,
    )
   
    student_model = GATModel(
        num_features=x.shape[1],
        num_classes=int(y.max().item()) + 1,
        hidden_units=args.student_hidden_units,
        num_heads=args.student_heads,
        output_layers=args.output_layers,
        hidden_units_output_layer=args.hidden_units_output_layer,
    )

    optimizer_teacher = optim.Adam(teacher_model.parameters(), lr=args.learning_rate)
    optimizer_student = optim.Adam(student_model.parameters(), lr=args.learning_rate)
    criterion = F.nll_loss

   
    def count_parameters(model):
     return sum(p.numel() for p in model.parameters() if p.requires_grad)
    print('*******Train Teacher********')

   
    train_loss = train_teacher(teacher_model, g, x, y, train_mask, optimizer_teacher, criterion,val_mask,test_mask,args.teacher_epochs)
      
 
    print(f'Number of parameters for teacher model: {count_parameters(teacher_model)}')
    with torch.no_grad():
        teacher_output = teacher_model(g, x)
    def count_parameters(model):
     return sum(p.numel() for p in model.parameters() if p.requires_grad)

    print('*******Train Student With Teacher********')

    train_loss, student_probs, ls_loss, gs_loss = train_student(
        args.tofull,args.mode,student_model, g, x, y, train_mask, optimizer_student, criterion,
        args.temperature, teacher_output, args.student_epochs, args.lambda_ls, args.beta_gs,val_mask,test_mask)
    print(f'Number of parameters for student model: {count_parameters(student_model)}')
  
      
    

if __name__ == "__main__":
    main()
